---
layout: post
title: LLM ç®—æ³•åˆ†æ
index_img: /img/gpt/index.jpg
date: 2024-08-07 13:49:09
archive: false
math: true
tags:
    - AI
    - GPU
    - LLM
categories: 
    - GPU
---

ä¸€æ–‡äº†è§£ GPT æ¨ç†åŸç†ã€‚

<!-- more -->

## AIæ¼”åŒ–è¶‹åŠ¿

AIæ¨¡å‹è§„æ¨¡è¶Šæ¥è¶Šåºå¤§ï¼Œè¶Šæ¥è¶Šç®€æ´ï¼Œäººå·¥å‚ä¸åº¦è¦æ±‚è¶Šæ¥è¶Šä½ï¼Œå’Œç¡¬ä»¶å‘å±•ç›¸åŒ¹é…ã€‚

- 2000å¹´å‰ç™¾å®¶é½æ”¾ï¼Œä¸»è¦ç¯å¢ƒPCæœºï¼š
  - ä¸“å®¶ç³»ç»Ÿã€è´å¶æ–¯ç½‘ç»œã€[æ”¯æŒå‘é‡æœº](https://charlesliuyx.github.io/2017/09/19/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/)ã€[å†³ç­–æ ‘](https://blog.csdn.net/weixin_46323666/article/details/125843236)ã€ç¥ç»ç½‘ç»œã€â€¦
  - IBMæ·±è“(å›½é™…è±¡æ£‹), Doctor Watson

{% gi 4 3-1 %}
    ![ä¸“å®¶ç³»ç»Ÿ](/img/gpt/expert.png)
    ![å†³ç­–æ ‘](/img/gpt/tree.png)
    ![å‘é‡æœº](/img/gpt/svm.png)
    ![æ›´æ·±çš„è“](/img/gpt/deepblue.jpg)
{% endgi %}

- 2000sæ·±åº¦å­¦ä¹ å…´èµ·ï¼Œå’ŒGPUé€‚é…
  - 1940sæ—©æœŸç¥ç»ç½‘ç»œï¼Œ1980s [CNN](https://yann.lecun.com/exdb/lenet/index.html)æå‡ºï¼Œ1990s RNN&[LSTM](https://developer.aliyun.com/article/1112196)ï¼Œå¤æ‚å¤šå˜çš„ç½‘ç»œç»“æ„å’Œç®—å­
    - [Lenetè®ºæ–‡å­¦ä¹ ç¬”è®°](https://blog.csdn.net/hduxiejun/article/details/53571768)
  - 2006 NV CUDAæ¡†æ¶ï¼Œ 2016 AlphaGo

{% gi 3 1-2 %}
    ![å·ç§¯ç¥ç»ç½‘ç»œ](/img/gpt/cnn.png)
    ![è´å¶æ–¯é•¿æœŸçŸ­æœŸè®°å¿†æ¨¡å‹ï¼ˆLSTMï¼‰](/img/gpt/LSTM.png)
    ![é˜¿æ³•ç‹—](/img/gpt/alphago.jpg)
{% endgi %}

- 2020så¤§è¯­è¨€æ¨¡å‹å…´èµ·, å¾—ç›ŠäºGPUä¸AIç›¸äº’ä¿ƒè¿›å‘å±•
  - 2017 **Attention is all you need**
  - 2018 GPT/BERT
  - 2022 chatGPT
    - **çŸ©é˜µä¹˜ï¼Œæ®‹å·®ç½‘ç»œï¼ŒLayerNormalizationï¼ŒAttention**
    - **Elementwise éçº¿æ€§æ¿€æ´»å‡½æ•°**
    - **åŸºäº Transformer ç»“æ„**

![LLM åº”ç”¨](/img/gpt/llmApp.png)

## ç¥ç»ç½‘ç»œç®€ä»‹

![ç¥ç»ç½‘ç»œ](/img/gpt/nn.png)

## æ®‹å·®ç¥ç»ç½‘ç»œ

è§£å†³é€€åŒ–é—®é¢˜

{% gi 2 2 %}
![Degradation Problem](/img/gpt/degradation-problem.png)
![å¤šå±‚æ®‹å·®ç¥ç»ç½‘ç»œ](/img/gpt/resnet.png)
{% endgi %}

è®¡ç®—æ®‹å·®:

![ç®—å­](/img/gpt/res.png)

## ç¼–ç å™¨ä¸è§£ç å™¨

Seq2Seq : RNN, LSTM, Transformer

- [REAL TIMESPEECH TRANSLATION USING RECURRENT NEURAL NETWORKS](https://ir.vignan.ac.in/id/eprint/623/1/19.ANAND%20REAL%20TIMESPEECH%20TRANSLATION%20USING%20RNN.pdf)

![Encode & Decoder](/img/gpt/encdec.gif)

## LLM åˆ†ç±»

{% gi 2 2 %}
  ![Attention æ¶æ„](/img/gpt/attention.png)
  ![åˆ†ç±»](/img/gpt/class.jpg)
{% endgi %}

## Encoder & DecoderåŠŸèƒ½å¯¹æ¯”

- Masked Language Model ï¼ˆBERT Trainingï¼‰
- Next Token Generation ï¼ˆGPTï¼‰ï¼šæˆ‘çˆ±åŒ—äº¬å¤©å®‰ï¼Œè®¡ç®—å‡ºä¸‹ä¸€ä¸ªå­— â€œé—¨â€

![bert](/img/gpt/bert.png)

## Attention ä»‹ç»

Attention is all you need!

![Bert VS GPT](/img/gpt/bertVSgpt.png)

$$
Attention=Softmax(QK^ğ‘‡)ğ‘‰
$$

![Attention](/img/gpt/qkv1.png)

å½’ä¸€åŒ–ï¼Œæƒé‡ç³»æ•°ï¼Œçªå‡ºä¸»è¦å› å­

$$
Softmax([ğ‘£1,ğ‘£2,ğ‘£3,â€¦,ğ‘£ğ‘›])= \frac{[ğ‘’^{ğ‘£1}, ğ‘’^{ğ‘£2}, ğ‘’^{ğ‘£3}, â€¦, ğ‘’^{ğ‘£3}]}{ğ‘’^{ğ‘£1} + ğ‘’^{ğ‘£2} + ğ‘’^{ğ‘£3} + â€¦ + ğ‘’^{ğ‘£ğ‘›}}
$$

ä¾‹å¦‚ï¼š

$$
Softmax([1,âˆ’âˆ,0,2,5])=[0.01704,0,0.00627,0.04632,0.93037]
$$

**Decode-only: æ¯ä¸ª Token éƒ½åªä¾èµ–å†å²ä¿¡æ¯è€Œä¸ä¾èµ–æœªæ¥ä¿¡æ¯ã€‚**å¯ä»¥é€šè¿‡ä½¿ç”¨ `KV-cache` æå¤§é™ä½generationè¿‡ç¨‹è®¡ç®—é‡ â€“ Memory Bound

æ›´å¤šï¼š

- [ä¸€æ–‡è®²é€é¢„è®­ç»ƒæ¨¡å‹çš„æ”¹è¿›è®­ç»ƒç®—æ³• ï¼Œè½»æ¾è¾¾åˆ°State of the Art](https://cloud.tencent.com/developer/article/1609905)
- [BERTæ¨¡å‹çš„è¯¦ç»†ä»‹ç»](https://blog.csdn.net/weixin_44799217/article/details/115374101)
- [transformer ä¸­: self-attention éƒ¨åˆ†æ˜¯å¦éœ€è¦è¿›è¡Œ maskï¼Ÿ](https://www.cvmart.net/community/detail/5137?hmsr=joyk.com&utm_source=joyk.com&utm_medium=referral)
- [å­¦ä¹ ç¬”è®°ï¼šåŸºäºTransformerçš„æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹](https://pythonziliao.com/post/758.html)
- [**Transformersæºç å­¦ä¹ **](https://qiankunli.github.io/2023/09/04/llm_source.html)
- [ä»self-attentionåˆ°transformerçš„è¶…è¯¦ç»†çš„ç®—æ³•è§£æå’Œä¸»æµè®ºæ–‡ç ”ç©¶åˆ†äº«](https://duanmofan.com/archives/self-attention2transformer)
- [This post is all you needï¼ˆâ‘ å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶åŸç†ï¼‰](https://juejin.cn/post/6982152969969991711)

## éçº¿æ€§æ¿€æ´»å‡½æ•°

![éçº¿æ€§å˜æ¢](/img/gpt/éçº¿æ€§å˜æ¢.png)

ç›®æ ‡ï¼Œé€šè¿‡éçº¿æ€§å˜æ¢æŠŠçš„åˆ†ç•Œçº¿æ‹‰ç›´ï¼›æ›´é«˜çš„ç»´åº¦æä¾›äº†æ›´å¤šçš„å¼¯æ›²å˜æ¢æœºä¼šã€‚

![GPT3 å…ˆå°†è¾“å…¥å‡ç»´åº¦å†é™ç»´åº¦](/img/gpt/ç»´åº¦.png)

Transformerä¸­ Feedforwad éƒ¨ä»¶ï¼ŒElementwise æ¿€æ´»å‡½æ•°ï¼ˆTensorä¸­æ¯ä¸ªå…ƒç´ ç‹¬ç«‹è¿›è¡Œç›¸åŒçš„éçº¿æ€§è¿ç®—ï¼‰

- GPT3ä½¿ç”¨ [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#relu) ç®—å­
- Llama2 ä½¿ç”¨[SwiGLU](https://zhuanlan.zhihu.com/p/650237644)ç®—å­ï¼Œå¸¦æœ‰å‚æ•°

{% gi 2 2 %}
  ![ReLU](/img/gpt/relu.png)
  ![SwiGLU](/img/gpt/swiglu.png)
{% endgi %}
