---
layout: post
title: "机密计算: 大模型安全"
index_img: /img/cc/LLM-cc.png
date: 2024-07-29 19:03:06
archive: true
tags:
    - LLM
    - TEE
    - Confidential Compute
    - Security
categories: 
    - Security
---

OpenAI 的 GPT 系列大语言模型的兴起与应用，也带来了诸如数据泄露、数据滥用、模型被攻击和知识产权被窃取等一系列隐私和安全风险或挑战。

<!-- more -->

## 机密计算

机密计算是一种安全和隐私增强的计算技术，专注于保护使用中的数据。机密计算可以与存储和网络加密结合使用，分别保护静态数据和传输中的数据。它旨在解决软件、协议、加密以及基本物理和供应链攻击，尽管一些批评者已经证明了对该技术有效的架构和旁道攻击。

![](/img/cc/LLM-cc-1.png)

机密计算的技术方法可能会有所不同，其中允许软件、基础设施和管理员元素访问机密数据。 “信任边界”界定了可信计算基础 (TCB)，定义了哪些元素有可能访问机密数据，无论它们是善意的还是恶意的。机密计算实现在特定的数据隔离级别上强制执行定义的信任边界。机密计算的三种主要类型是：

- 虚拟机隔离。虚拟机隔离删除了由计算机基础设施或云提供商控制的元素，但允许基础设施上运行的虚拟机内部的元素访问潜在的数据
- 应用程序隔离，也称为进程隔离。应用程序或进程隔离仅允许授权的软件应用程序或进程访问数据
- 函数隔离，也称为库隔离。函数或库隔离旨在仅允许大型应用程序中的授权子例程或模块访问数据，阻止任何其他系统元素（包括大型应用程序中未经授权的代码）的访问

### 威胁模型

参考：[A Technical Analysis of Confidential Computing v1.3](https://confidentialcomputing.io/wp-content/uploads/sites/10/2023/03/CCC-A-Technical-Analysis-of-Confidential-Computing-v1.3_unlocked.pdf)
以下威胁向量通常被视为机密计算的范围：

- 软件攻击：包括对主机软件和固件的攻击。这可能包括操作系统、管理程序、BIOS、其他软件和工作负载。
- 协议攻击：包括“对与证明以及工作负载和数据传输相关的协议的攻击”。这包括“工作负载的配置或放置”或可能导致泄露的数据中的漏洞。
- 加密攻击：包括“由于多种因素而在密码和算法中发现的漏洞，包括数学突破、计算能力的可用性以及量子计算等新计算方法”。 CCC 指出了此威胁向量中的几个警告，包括升级硬件中的加密算法的相对困难以及软件和固件保持最新的建议。建议将多方面的纵深防御策略作为最佳实践。
- 基本物理攻击：包括冷启动攻击、总线和缓存窥探以及将攻击设备插入现有端口（例如 PCI Express 插槽或 USB 端口）。
- 基本本上游供应链攻击：包括通过添加调试端口等更改来危害 TEE 的攻击。

通常定义为超出机密计算范围的威胁包括：

- 复杂的物理攻击：包括“需要长期和/或侵入性访问硬件”的物理攻击，例如芯片刮擦技术和电子显微镜探针。
- 上游硬件供应链攻击：包括对CPU制造过程、制造期间密钥注入/生成中的CPU供应链的攻击。对不直接提供可信执行环境功能的主机系统组件的攻击通常也超出了范围。
- 可用性攻击：机密计算旨在保护受保护数据和代码的机密性和完整性。它不解决可用性攻击，例如拒绝服务或分布式拒绝服务攻击。

## TEE and LLM

### 大模型安全隐患

数据泄露、数据滥用、模型被攻击和知识产权被窃取等一系列隐私和安全风险。

- 数据泄露：在公共云中使用 LLM 时，必须考虑用于推理的数据的敏感性。
- 数据滥用：实施数据清理和验证技术，以确保提示不会无意中泄露敏感信息
- 模型攻击：实施强大的访问控制来限制谁可以访问和使用经过微调的模型

### 可信执行环境机制

OpenAI 的 GPT 系列大语言模型（Large Language Mode，以下缩写为 LLM）的兴起与应用，也带来了诸如数据泄露、数据滥用、模型被攻击和知识产权被窃取等一系列隐私和安全风险或挑战。

可信执行环境（Trusted Execution Environment，以下缩写为 TEE）是一项基于软硬件组合创建安全执行环境，能够更好地确保计算和数据处理机密性和完整性。其关键机制为：

- 安全隔离：通过硬件加密和内存隔离等硬件隔离技术，将敏感数据和关键代码与其他应用及操作系统相隔离，从而确保它们即使在系统其他部分被攻击或受到恶意软件影响时也能够得到更好的保护。
- 安全验证：在启动过程中进行身份验证和完整性检查，确保只有经过授权的代码和数据可以在其中运行，以此防止恶意软件或未经授权的访问。
- 安全执行环境：提供包含加密算法、安全协议和密钥管理等防护功能的执行环境，用于处理敏感数据和执行关键算法，以增强数据在执行过程中的保密性和完整性。

TEE 与 LLM 可在多行业、多场景融合，TEE 可用于为 LLM 提供颇具商业落地价值的隐私和数据保护创新解决方案。

### LLM和TEE融合需求

LLM 在许多行业的不同场景都有着广泛应用，例如金融行业的风险评估和交易分析，医疗保健领域的医学图像识别、病历纪录和疾病预测，以及法律和合规行业的法律咨询、合同审查和文书处理等。这些行业或场景中涉及到的数据多为重要敏感的交易数据或个人数据，必须得到有效保护。

将 TEE 与 LLM 融合，有助于在这类场景中更好地保障数据在 LLM 模型训练和推理过程中的保密性。训练阶段，TEE 中的数据处理都处于加密状态；推理阶段，TEE 则可保护用户输入和模型结果的隐私。同时，其硬件隔离和安全验证机制可以更有效地防止未经授权的访问和攻击，增强模型运行时的安全性。

- 训练阶段：允许用户使用特定领域或语言的专有数据来培训 LLM，而无需与其他人共享。经过训练的模型和数据保持私密性，只有容器内的用户和 LLM 可以访问。此外，用户可以确认代码和参数没有被其他人更改。
- 推理阶段：TEE 则可保护用户输入和模型结果的隐私。
- 微调阶段：可以使用机密容器针对特定领域的任务在其特定数据集上微调预训练的 LLM，确保只有机密容器内的用户才能访问数据和模型。

仅在成功验证运行环境的完整性（使用 远程证明）后，机密才会在机密容器内传递。由于解密的模型和代码在 TEE 内部运行，因此任何不受信任的实体都无法对其进行未经授权的访问。

### TEE和LLM融合的挑战: 资源和性能限制

- 资源限制：TEE 的计算资源和存储空间通常都非常有限，LLM 庞大的模型参数和计算需求可能会超出一般 TEE 的能力范围。
- 性能下降：I/O 数据的加密和安全计算操作会引入额外的计算开销，导致模型训练和推理性能有一定程度下降。基于算法的解决方案可减少模型规模和计算需求，以适应 TEE 的资源限制，但 CPU 仍会成为制约 LLM 训练的算力瓶颈。

## Intel平台加速TEE和LLM融合方案

### SGX/TDX解决方案

英特尔自第三代英特尔至强可扩展处理器开始内置英特尔软件防护扩展（英特尔SGX）技术，其安全飞地的容量最多可达单颗 CPU 512GB，双路共计 1TB 容量，可满足目前千亿大模型的执行空间需求。此外，该技术提供支持的机密计算可实现应用层、虚拟机 (VM)、容器和功能层的数据隔离。无论是在云端、边缘还是本地环境，都能确保计算与数据始终在私密性和安全性上获得更全面的保护，以免暴露给云服务提供商、未经授权的管理员和操作系统，甚至是特权应用。另一方面，英特尔Trust Domain Extension（英特尔TDX）可将客户机操作系统和虚拟机应用与云端主机、系统管理程序和平台的其他虚拟机隔离开来。它的信任边界较英特尔SGX 应用层的隔离边界更大，使受其保护的机密虚拟机比基于英特尔SGX 的安全飞地的应用更易于进行大规模部署和管理，在部署 LLM 这类复杂应用时，TDX 在易用性上更具优势。此外，今年推出的全新第四代英特尔至强可扩展处理器内置英特尔AMX，可大幅提升矩阵运算性能，而英特尔SGX/TDX也可为英特尔AMX、英特尔DL Boost等计算指令提供支持，进而为 TEE 中的大模型赋予快速落地+优化性能的双重优势。

![SGX/TDX 的可信边界](/img/cc/LLM-cc-2.png)

构建完善的 TEE 生态系统对推动 LLM 的应用和发展至关重要。开发者需要能够简化集成和使用过程的面向 TEE 的开发者工具和框架。为此，英特尔在 SDK 的基础上，推出了开源的 lib OS 项目 Gramine 来帮助开发者更好地使用基于英特尔SGX的 TEE，助推 LLM 与 TEE 的融合。

### 大语言模型推理

使用私有数据进行个性化训练的大模型不仅包含私有数据信息，其查询本身也具有隐私性，尤其是基于边端的非安全环境部署。基于英特尔SGX/TDX 的 TEE 可为大模型提供更安全的运行环境，在数据上传云端前，查询可先通过客户端对传输内容加密，云端只需在英特尔SGX/TDX 中解密查询问题，然后输入大模型的推理服务中，并将所得结果在云端的 TEE 中加密后传输回本地客户端。在整个工作流程中，客户端以外的数据和运行态程序均处于密态环境当中，效率远远高于其他基于纯密码学的解决方案。目前像 LLAMA 7B、ChatGLM 6B 等模型都可以在该 TEE 方案上满足实时可交互性能的运行。图展示了使用 LLM 部署知识问答的参考设计。基于英特尔SGX/TDX 的 TEE 为实际部署 LLM 中的自有知识产权保护提供了一套完整的方案，优化整个模型在查询、传输和推理过程中的安全保护。

![基于 TEE 的大语言模型私密问答](/img/cc/LLM-cc-3.png)

### 联邦学习

借助基于 TEE 的联邦学习解决方案（Wei Yu. et al. 2022, TEE based Cross-silo Trustworthy Federated Learning Infrastructure, FL-IJCAI'22），就可在多机构之间实现基于 NLP 的深度学习，例如使用 BERT 的命名体识别。在金融和医疗等行业提升准确性，实现多机构数据互通，同时更好避免数据泄露。

此方案中每个参与方包含一个 [Avalon](https://github.com/hyperledger-archives/avalon) 管理模块和 Gramine 工作负载，均运行在英特尔SGX 的安全飞地中，在管理模块彼此间的远程认证完成执行后，即可启动联邦学习过程，参与方在本地使用各自的数据集进行训练，然后将梯度上传至聚合方，聚合方进行聚合后将平均梯度下发至各参与方，以继续进行下一轮训练。对比图所示的 [BERT + CRF 模型 Souza, F., Nogueira, R. and Lotufo, R. 2020, Portuguese Named Entity Recognition using Bert-CRF, arXiv.org](https://arxiv.org/pdf/1909.10649.pdf)，此方案可以在强化隐私保护的同时，让性能损失维持在 50% 以下2。

![基于 TEE 的联邦学习](/img/cc/LLM-cc-4.png)
![BERT + CRF 模型](/img/cc/LLM-cc-5.png)

### BigDL：端到端大模型和 TEE 融合的方案

据行业用户反馈，LLM 在端到端应用中的痛点包括：

- 软件栈复杂，难以确保端到端应用的安全。LLM 的训练和推理常依赖较多的软件栈、服务和硬件。为保护用户数据和模型产权，需确保每个环节的安全性（不同硬件、运行环境、网络和存储等）。
- 计算量大，且对性能敏感。LLM 的计算量非常大，需引入足够多的性能优化。但是，不同模型、平台和软件栈需要使用不同的优化方案，要在特定平台上实现更理想的性能，需要长时间的性能调优。

为解决这些痛点，由**英特尔主导的开源项目 BigDL**，近期就推出了针对 LLM 的隐私保护方案，其两大主要功能为：

- 提供端到端的安全保护：在不修改代码的情况下，为单机和分布式的 LLM 应用提供端到端的安全保护功能。具体包括，基于英特尔SGX/TDX 的 TEE、远程证明、统一的密钥管理接口和透明的加解密 API 等。
- 实现一站式性能优化：BigDL Nano 提供的针对 LLM 的一站式性能优化方案，可让现有 LLM 应用在几乎不用修改代码的情况下受益于英特尔AMX、英特尔AVX-512 和英特尔Extension for PyTorch。同时，用户还可利用 BigDL Nano 提供的 LLM API，快速构建应用。
- 
![BigDL 端到端安全的大模型方案](/img/cc/LLM-cc-6.png)

如下图所示，在应用了 PPML（Privacy Preserving Machine Learning，隐私保护的机器学习）提供的安全技术后，由于更强的安全和隐私保护会带来额外开销，因此端到端应用性能会略有下降；但应用了 BigDL Nano 提供的优化功能后，端到端的性能得到了显著改善*，总体性能甚至高于没有任何保护的明文性能。

![BigDL PPML + Nano 端到端性能损失情况](/img/cc/LLM-cc-7.png)

目前，该方案已经开源，并开始陆续交付给行业客户进行测试和集成[Github](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm)。

### 未来趋势

TEE 提供了隐私保护和数据安全防护功能的创新解决方案，将在 LLM 实际落地过程中扮演重要角色。通过将二者融合，可更好地保障数据在训练和推理过程中的保密性，增强对未经授权访问和模型结果篡改的防御。然而，在 TEE 中保护用户隐私的同时，需要平衡性能需求，随着大模型对于计算需求的大幅提升，算力可能会执行在异构硬件上，TEE 和异构硬件的结合将成为未来发展趋势。随着 CPU 性能的提升以及内置 AI 加速技术的升级和更新，在方便部署的场景下，CPU 会是大模型推理和 TEE 结合的首选，在训练的场景下，基于 CPU 的 TEE 结合异构硬件的加密支持，则会是大模型训练甚至大模型联邦训练的技术方向。英特尔将一如既往地以软硬结合的产品技术组合促进开发者参与，推动 LLM 与 TEE 的融合。

## OpaquePrompts隐藏敏感输入数据

### 动机

- OpenAI 面向消费者的 ChatGPT不断根据用户输入训练其模型，并与第三方提供商共享用户输入。
- Anthropic 的 Claude 默认保留我们的数据至少 30 天。
- Google 的 Bard 会将我们的活动保留至少 3 个月，最长可能保留 3 年。它还使用输入数据来持续训练其模型。

通过在个人或敏感数据到达 LLM 之前对其进行清理，不再需要依赖 LLM 提供商来遵循任何特定的数据保留或处理政策。这就是 OpaquePrompts 提供的功能。通过利用机密计算和可信执行环境 (TEE)，托管 OpaquePrompts 的 Opaque 也不会看到提示，从而确保只有用户才能看到prompt中包含的任何信息。

### OpaquePrompts

工作流程如下：

![](/img/cc/LLM-cc-8.png)

- 给定用户提供的输入，LLM 应用程序会像以前一样构建提示，可能包括检索到的上下文、内存和用户查询。LLM 应用程序将此提示转发给 OpaquePrompts。
- 在 TEE 中，OpaquePrompts 使用基于自然语言处理 (NLP) 的机器学习来识别提示中的敏感标记。
- 在 TEE 中，OpaquePrompts 通过加密所有个人和敏感令牌来清理（换句话说，加密和编辑）提示，然后再将清理后的提示返回到 LLM 应用程序。
- LLM 申请将提示提交给他们选择的 LLM（例如 ChatGPT 或 Claude），LLM 将清理响应（包含具有个人信息的令牌的清理版本的响应）返回给 LLM 应用程序。
- 在 TEE 中，OpaquePrompts 从 LLM 应用程序接收经过净化的响应并对其进行去净化，用其明文等效项替换经过净化的令牌。
- LLM 应用程序将经过净化的响应返回给用户。

借助 OpaquePrompts，LLM 申请者在使用第三方 LLM 之前不再需要依赖用户手动删除任何敏感信息。

## H100

- 基于硬件的安全和隔离：在本地、云端或边缘实现虚拟机 (VM) 的完全隔离。CPU 和 H200 或 H100 GPU 之间的数据传输以 PCIe 线速加密和解密。大部分 GPU 内存被配置为计算保护区 (CPR)，使用内置硬件防火墙创建物理隔离的可信执行环境 (TEE)，以保护 H200 或 H100 GPU 上的整个工作负载。
- 防止未经授权的访问：保护使用中的数据和人工智能工作负载的机密性和完整性。未经授权的实体（包括虚拟机管理程序、主机操作系统、云提供商）无法在执行期间查看或修改人工智能应用程序和数据。
- 设备证明的可验证性：确保只有授权的最终用户才能在 H200 或 H100 的 TEE 中放置要执行的数据和代码。此外，设备认证可验证用户是否正在与真实的 NVIDIA GPU 进行通信、固件是否未被篡改以及 GPU 固件是否已按预期更新。
- 无需更改应用程序代码：在大多数情况下， GPU 加速工作负载无需更改代码，即可充分利用机密计算的所有优势，同时保持安全性、隐私性。


## 安全可信的LLM

在 TEE 中保护用户隐私的同时，需要平衡性能需求，TEE 和异构硬件的结合将成为未来发展趋势。 随着 CPU 性能的提升以及内置 AI 加速技术的升级和更新，在方便部署的场景下，CPU 会是大模型推理和 TEE 结合的首选；在训练的场景下，基于 CPU 的 TEE 结合异构硬件的加密支持，则会是大模型训练甚至大模型联邦训练的技术方向。

- 中国信通院《大模型可信赖研究报告》提出了大模型可信赖六大目标，包括可靠性、健壮性、安全性、公平性、可问责和可解释，同时提出大模型可信风险来自于框架、数据、模型和生成内容等四大方面。
- AI数据标注服务是大模型生命周期中的重要一环。Appen是一家全球领先的AI数据标注服务商，提供AI数据采集和数据标注服务。Appen在全球拥有百万数据标注众包资源，该公司在语音数据标、图像数据标注、自动驾驶数据标注等领域具有独特优势。进入大模型时代，Appen也推出了基于人类反馈的强化学习等数据和AI服务，用于解决大语言模型中存在偏见和幻觉等风险，还推出了自动化NLP数据标注，利用生成式AI功能和零样本/少样本学习技术加快数据标注
- 大模型AI安全可信，是一组相关数据、技术、产品、方案和服务等组合起来的能力集合，其中包括：数据采集、标注和清洗等数据服务；模型设计、评估与微调等模型服务；基于人类反馈的增强学习服务；数据保护与隐私安全；咨询服务等等诸多领域，涉及到了大模型全生命周期的方方面面。
- 在大模型公有云服务方面，以百度、阿里等为代表的互联网与云服务公司，从大模型全生命周期视角出发，涵盖大模型训练、精调、推理、大模型部署、大模型运营等关键阶段面临的安全风险与业务挑战，在自有技术体系内进行深入布局，探索打造安全产品与服务。
- 360等第三方独立的人工智能与安全科技公司，探索“以模型管理模型”方式，打造以大模型为核心的AI Agent（AI智能体）,带入企业真实安全运营场景中，以“虚拟安全专家”的形象，满足企业对安全业务的需求。
- IBM等公司推出了独立的AI安全可信工具或组件，专门用于检查数据集、机器学习模型、业界最新算法等是否具有偏见；还在一些组件中专门集成了AI模型风险管理以及数据治理和数据质量管理等功能。
- 开源社区也积极推出大模型安全可信开源工具。[LLM Guard](https://github.com/protectai/llm-guard)是一个保护和强化大型语言模型安全性的开源工具包，LLM Guard的目的是通过提供开箱即用的所有必要工具来简化公司安全采用大模型的过程。

## 参考

[可信执行环境保障大模型安全](https://blog.csdn.net/qq_43543209/article/details/135683986)
[如何构建安全可信的AI大模型](https://mp.weixin.qq.com/s/6hdBb0fDQUZIQiHjEnpG5g)
[用基于英特尔SGX 的可信执行环境有效应对大语言模型隐私和安全挑战](https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/privacy-security-challenge-large-language-model.html)
[维基百科 Confidential computing 机密计算](https://en.wikipedia.org/wiki/Confidential_computing)
[维基百科 安全多方计算](https://en.wikipedia.org/wiki/Secure_multi-party_computation)
[机密计算联盟](https://confidentialcomputing.io/resources/white-papers-reports/)
[7 个服务框架 LLMs](https://betterprogramming.pub/frameworks-for-serving-llms-60b7f7b23407)
[保护 Kubernetes 工作负载的安全：签名和加密容器映像的实用方法](https://itnext.io/securing-kubernetes-workloads-a-practical-approach-to-signed-and-encrypted-container-images-ff6e98b65bcd)
[大型语言模型的机密容器](https://pradiptabanerjee.medium.com/confidential-containers-for-large-language-models-42477436345a)
[LLM Guard - 大规模语言模型交互的安全卫士](https://blog.csdn.net/gitblog_00056/article/details/139085060)
