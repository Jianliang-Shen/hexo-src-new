---
layout: post
title: "机密计算: 大模型安全"
index_img: /img/post_pics/cc/LLM-cc.png
date: 2024-07-18 11:29:33
tags:
    - LLM
    - TEE
    - Confidential Compute
    - Security
categories: 
    - Security
---

OpenAI 的 GPT 系列大语言模型的兴起与应用，也带来了诸如数据泄露、数据滥用、模型被攻击和知识产权被窃取等一系列隐私和安全风险或挑战。

<!-- more -->

## 机密计算

机密计算是一种安全和隐私增强的计算技术，专注于保护使用中的数据。机密计算可以与存储和网络加密结合使用，分别保护静态数据和传输中的数据。它旨在解决软件、协议、加密以及基本物理和供应链攻击，尽管一些批评者已经证明了对该技术有效的架构和旁道攻击。

![](/img/post_pics/cc/LLM-cc-1.png)

机密计算的技术方法可能会有所不同，其中允许软件、基础设施和管理员元素访问机密数据。 “信任边界”界定了可信计算基础 (TCB)，定义了哪些元素有可能访问机密数据，无论它们是善意的还是恶意的。机密计算实现在特定的数据隔离级别上强制执行定义的信任边界。机密计算的三种主要类型是：

- 虚拟机隔离。虚拟机隔离删除了由计算机基础设施或云提供商控制的元素，但允许基础设施上运行的虚拟机内部的元素访问潜在的数据
- 应用程序隔离，也称为进程隔离。应用程序或进程隔离仅允许授权的软件应用程序或进程访问数据
- 函数隔离，也称为库隔离。函数或库隔离旨在仅允许大型应用程序中的授权子例程或模块访问数据，阻止任何其他系统元素（包括大型应用程序中未经授权的代码）的访问

### 威胁模型

参考：[A Technical Analysis of Confidential Computing v1.3](https://confidentialcomputing.io/wp-content/uploads/sites/10/2023/03/CCC-A-Technical-Analysis-of-Confidential-Computing-v1.3_unlocked.pdf)
以下威胁向量通常被视为机密计算的范围：

- 软件攻击：包括对主机软件和固件的攻击。这可能包括操作系统、管理程序、BIOS、其他软件和工作负载。
- 协议攻击：包括“对与证明以及工作负载和数据传输相关的协议的攻击”。这包括“工作负载的配置或放置”或可能导致泄露的数据中的漏洞。
- 加密攻击：包括“由于多种因素而在密码和算法中发现的漏洞，包括数学突破、计算能力的可用性以及量子计算等新计算方法”。 CCC 指出了此威胁向量中的几个警告，包括升级硬件中的加密算法的相对困难以及软件和固件保持最新的建议。建议将多方面的纵深防御策略作为最佳实践。
- 基本物理攻击：包括冷启动攻击、总线和缓存窥探以及将攻击设备插入现有端口（例如 PCI Express 插槽或 USB 端口）。
基- 本上游供应链攻击：包括通过添加调试端口等更改来危害 TEE 的攻击。

通常定义为超出机密计算范围的威胁包括：

- 复杂的物理攻击：包括“需要长期和/或侵入性访问硬件”的物理攻击，例如芯片刮擦技术和电子显微镜探针。
- 上游硬件供应链攻击：包括对CPU制造过程、制造期间密钥注入/生成中的CPU供应链的攻击。对不直接提供可信执行环境功能的主机系统组件的攻击通常也超出了范围。
- 可用性攻击：机密计算旨在保护受保护数据和代码的机密性和完整性。它不解决可用性攻击，例如拒绝服务或分布式拒绝服务攻击。

## TEE and LLM

### 大模型安全隐患

数据泄露、数据滥用、模型被攻击和知识产权被窃取等一系列隐私和安全风险。

- 数据泄露：在公共云中使用 LLM 时，必须考虑用于推理的数据的敏感性。
- 数据滥用：实施数据清理和验证技术，以确保提示不会无意中泄露敏感信息
- 模型攻击：实施强大的访问控制来限制谁可以访问和使用经过微调的模型

### 可信执行环境机制

OpenAI 的 GPT 系列大语言模型（Large Language Mode，以下缩写为 LLM）的兴起与应用，也带来了诸如数据泄露、数据滥用、模型被攻击和知识产权被窃取等一系列隐私和安全风险或挑战。

可信执行环境（Trusted Execution Environment，以下缩写为 TEE）是一项基于软硬件组合创建安全执行环境，能够更好地确保计算和数据处理机密性和完整性。其关键机制为：

- 安全隔离：通过硬件加密和内存隔离等硬件隔离技术，将敏感数据和关键代码与其他应用及操作系统相隔离，从而确保它们即使在系统其他部分被攻击或受到恶意软件影响时也能够得到更好的保护。
- 安全验证：在启动过程中进行身份验证和完整性检查，确保只有经过授权的代码和数据可以在其中运行，以此防止恶意软件或未经授权的访问。
- 安全执行环境：提供包含加密算法、安全协议和密钥管理等防护功能的执行环境，用于处理敏感数据和执行关键算法，以增强数据在执行过程中的保密性和完整性。

TEE 与 LLM 可在多行业、多场景融合，TEE 可用于为 LLM 提供颇具商业落地价值的隐私和数据保护创新解决方案。

### LLM和TEE融合需求

LLM 在许多行业的不同场景都有着广泛应用，例如金融行业的风险评估和交易分析，医疗保健领域的医学图像识别、病历纪录和疾病预测，以及法律和合规行业的法律咨询、合同审查和文书处理等。这些行业或场景中涉及到的数据多为重要敏感的交易数据或个人数据，必须得到有效保护。

将 TEE 与 LLM 融合，有助于在这类场景中更好地保障数据在 LLM 模型训练和推理过程中的保密性。训练阶段，TEE 中的数据处理都处于加密状态；推理阶段，TEE 则可保护用户输入和模型结果的隐私。同时，其硬件隔离和安全验证机制可以更有效地防止未经授权的访问和攻击，增强模型运行时的安全性。

- 训练阶段：允许用户使用特定领域或语言的专有数据来培训 LLM，而无需与其他人共享。经过训练的模型和数据保持私密性，只有容器内的用户和 LLM 可以访问。此外，用户可以确认代码和参数没有被其他人更改。
- 推理阶段：TEE 则可保护用户输入和模型结果的隐私。
- 微调阶段：可以使用机密容器针对特定领域的任务在其特定数据集上微调预训练的 LLM，确保只有机密容器内的用户才能访问数据和模型。

仅在成功验证运行环境的完整性（使用 远程证明）后，机密才会在机密容器内传递。由于解密的模型和代码在 TEE 内部运行，因此任何不受信任的实体都无法对其进行未经授权的访问。

### TEE和LLM融合的挑战: 资源和性能限制

- 资源限制：TEE 的计算资源和存储空间通常都非常有限，LLM 庞大的模型参数和计算需求可能会超出一般 TEE 的能力范围。
- 性能下降：I/O 数据的加密和安全计算操作会引入额外的计算开销，导致模型训练和推理性能有一定程度下降。基于算法的解决方案可减少模型规模和计算需求，以适应 TEE 的资源限制，但 CPU 仍会成为制约 LLM 训练的算力瓶颈。

## Intel平台加速TEE和LLM融合方案

### SGX/TDX解决方案

英特尔自第三代英特尔至强可扩展处理器开始内置英特尔软件防护扩展（英特尔SGX）技术，其安全飞地的容量最多可达单颗 CPU 512GB，双路共计 1TB 容量，可满足目前千亿大模型的执行空间需求。此外，该技术提供支持的机密计算可实现应用层、虚拟机 (VM)、容器和功能层的数据隔离。无论是在云端、边缘还是本地环境，都能确保计算与数据始终在私密性和安全性上获得更全面的保护，以免暴露给云服务提供商、未经授权的管理员和操作系统，甚至是特权应用。另一方面，英特尔Trust Domain Extension（英特尔TDX）可将客户机操作系统和虚拟机应用与云端主机、系统管理程序和平台的其他虚拟机隔离开来。它的信任边界较英特尔SGX 应用层的隔离边界更大，使受其保护的机密虚拟机比基于英特尔SGX 的安全飞地的应用更易于进行大规模部署和管理，在部署 LLM 这类复杂应用时，TDX 在易用性上更具优势。此外，今年推出的全新第四代英特尔至强可扩展处理器内置英特尔AMX，可大幅提升矩阵运算性能，而英特尔SGX/TDX也可为英特尔AMX、英特尔DL Boost等计算指令提供支持，进而为 TEE 中的大模型赋予快速落地+优化性能的双重优势。

![SGX/TDX 的可信边界](/img/post_pics/cc/LLM-cc-2.png)

构建完善的 TEE 生态系统对推动 LLM 的应用和发展至关重要。开发者需要能够简化集成和使用过程的面向 TEE 的开发者工具和框架。为此，英特尔在 SDK 的基础上，推出了开源的 lib OS 项目 Gramine 来帮助开发者更好地使用基于英特尔SGX的 TEE，助推 LLM 与 TEE 的融合。

### 大语言模型推理

使用私有数据进行个性化训练的大模型不仅包含私有数据信息，其查询本身也具有隐私性，尤其是基于边端的非安全环境部署。基于英特尔SGX/TDX 的 TEE 可为大模型提供更安全的运行环境，在数据上传云端前，查询可先通过客户端对传输内容加密，云端只需在英特尔SGX/TDX 中解密查询问题，然后输入大模型的推理服务中，并将所得结果在云端的 TEE 中加密后传输回本地客户端。在整个工作流程中，客户端以外的数据和运行态程序均处于密态环境当中，效率远远高于其他基于纯密码学的解决方案。目前像 LLAMA 7B、ChatGLM 6B 等模型都可以在该 TEE 方案上满足实时可交互性能的运行。图展示了使用 LLM 部署知识问答的参考设计。基于英特尔SGX/TDX 的 TEE 为实际部署 LLM 中的自有知识产权保护提供了一套完整的方案，优化整个模型在查询、传输和推理过程中的安全保护。

![基于 TEE 的大语言模型私密问答](/img/post_pics/cc/LLM-cc-3.png)

### 联邦学习

借助基于 TEE 的联邦学习解决方案（Wei Yu. et al. 2022, TEE based Cross-silo Trustworthy Federated Learning Infrastructure, FL-IJCAI'22），就可在多机构之间实现基于 NLP 的深度学习，例如使用 BERT 的命名体识别。在金融和医疗等行业提升准确性，实现多机构数据互通，同时更好避免数据泄露。

此方案中每个参与方包含一个 [Avalon](https://github.com/hyperledger-archives/avalon) 管理模块和 Gramine 工作负载，均运行在英特尔SGX 的安全飞地中，在管理模块彼此间的远程认证完成执行后，即可启动联邦学习过程，参与方在本地使用各自的数据集进行训练，然后将梯度上传至聚合方，聚合方进行聚合后将平均梯度下发至各参与方，以继续进行下一轮训练。对比图所示的 [BERT + CRF 模型 Souza, F., Nogueira, R. and Lotufo, R. 2020, Portuguese Named Entity Recognition using Bert-CRF, arXiv.org](https://arxiv.org/pdf/1909.10649.pdf)，此方案可以在强化隐私保护的同时，让性能损失维持在 50% 以下2。

![基于 TEE 的联邦学习](/img/post_pics/cc/LLM-cc-4.png)
![BERT + CRF 模型](/img/post_pics/cc/LLM-cc-5.png)

### BigDL：端到端大模型和 TEE 融合的方案

据行业用户反馈，LLM 在端到端应用中的痛点包括：

- 软件栈复杂，难以确保端到端应用的安全。LLM 的训练和推理常依赖较多的软件栈、服务和硬件。为保护用户数据和模型产权，需确保每个环节的安全性（不同硬件、运行环境、网络和存储等）。
- 计算量大，且对性能敏感。LLM 的计算量非常大，需引入足够多的性能优化。但是，不同模型、平台和软件栈需要使用不同的优化方案，要在特定平台上实现更理想的性能，需要长时间的性能调优。

为解决这些痛点，由**英特尔主导的开源项目 BigDL**，近期就推出了针对 LLM 的隐私保护方案，其两大主要功能为：

- 提供端到端的安全保护：在不修改代码的情况下，为单机和分布式的 LLM 应用提供端到端的安全保护功能。具体包括，基于英特尔SGX/TDX 的 TEE、远程证明、统一的密钥管理接口和透明的加解密 API 等。
- 实现一站式性能优化：BigDL Nano 提供的针对 LLM 的一站式性能优化方案，可让现有 LLM 应用在几乎不用修改代码的情况下受益于英特尔AMX、英特尔AVX-512 和英特尔Extension for PyTorch。同时，用户还可利用 BigDL Nano 提供的 LLM API，快速构建应用。
- 
![BigDL 端到端安全的大模型方案](/img/post_pics/cc/LLM-cc-6.png)

如下图所示，在应用了 PPML（Privacy Preserving Machine Learning，隐私保护的机器学习）提供的安全技术后，由于更强的安全和隐私保护会带来额外开销，因此端到端应用性能会略有下降；但应用了 BigDL Nano 提供的优化功能后，端到端的性能得到了显著改善*，总体性能甚至高于没有任何保护的明文性能。

![BigDL PPML + Nano 端到端性能损失情况](/img/post_pics/cc/LLM-cc-7.png)

目前，该方案已经开源，并开始陆续交付给行业客户进行测试和集成[Github](https://github.com/intel-analytics/ipex-llm/tree/main/python/llm)。

### 未来趋势

TEE 提供了隐私保护和数据安全防护功能的创新解决方案，将在 LLM 实际落地过程中扮演重要角色。通过将二者融合，可更好地保障数据在训练和推理过程中的保密性，增强对未经授权访问和模型结果篡改的防御。然而，在 TEE 中保护用户隐私的同时，需要平衡性能需求，随着大模型对于计算需求的大幅提升，算力可能会执行在异构硬件上，TEE 和异构硬件的结合将成为未来发展趋势。随着 CPU 性能的提升以及内置 AI 加速技术的升级和更新，在方便部署的场景下，CPU 会是大模型推理和 TEE 结合的首选，在训练的场景下，基于 CPU 的 TEE 结合异构硬件的加密支持，则会是大模型训练甚至大模型联邦训练的技术方向。英特尔将一如既往地以软硬结合的产品技术组合促进开发者参与，推动 LLM 与 TEE 的融合。

## OpaquePrompts隐藏敏感输入数据

### 动机

- OpenAI 面向消费者的 ChatGPT不断根据用户输入训练其模型，并与第三方提供商共享用户输入。
- Anthropic 的 Claude 默认保留我们的数据至少 30 天。
- Google 的 Bard 会将我们的活动保留至少 3 个月，最长可能保留 3 年。它还使用输入数据来持续训练其模型。

通过在个人或敏感数据到达 LLM 之前对其进行清理，不再需要依赖 LLM 提供商来遵循任何特定的数据保留或处理政策。这就是 OpaquePrompts 提供的功能。通过利用机密计算和可信执行环境 (TEE)，托管 OpaquePrompts 的 Opaque 也不会看到提示，从而确保只有用户才能看到prompt中包含的任何信息。

### OpaquePrompts

工作流程如下：

![](/img/post_pics/cc/LLM-cc-8.png)

- 给定用户提供的输入，LLM 应用程序会像以前一样构建提示，可能包括检索到的上下文、内存和用户查询。LLM 应用程序将此提示转发给 OpaquePrompts。
- 在 TEE 中，OpaquePrompts 使用基于自然语言处理 (NLP) 的机器学习来识别提示中的敏感标记。
- 在 TEE 中，OpaquePrompts 通过加密所有个人和敏感令牌来清理（换句话说，加密和编辑）提示，然后再将清理后的提示返回到 LLM 应用程序。
- LLM 申请将提示提交给他们选择的 LLM（例如 ChatGPT 或 Claude），LLM 将清理响应（包含具有个人信息的令牌的清理版本的响应）返回给 LLM 应用程序。
- 在 TEE 中，OpaquePrompts 从 LLM 应用程序接收经过净化的响应并对其进行去净化，用其明文等效项替换经过净化的令牌。
- LLM 应用程序将经过净化的响应返回给用户。

借助 OpaquePrompts，LLM 申请者在使用第三方 LLM 之前不再需要依赖用户手动删除任何敏感信息。

## H100

- 基于硬件的安全和隔离：在本地、云端或边缘实现虚拟机 (VM) 的完全隔离。CPU 和 H200 或 H100 GPU 之间的数据传输以 PCIe 线速加密和解密。大部分 GPU 内存被配置为计算保护区 (CPR)，使用内置硬件防火墙创建物理隔离的可信执行环境 (TEE)，以保护 H200 或 H100 GPU 上的整个工作负载。
- 防止未经授权的访问：保护使用中的数据和人工智能工作负载的机密性和完整性。未经授权的实体（包括虚拟机管理程序、主机操作系统、云提供商）无法在执行期间查看或修改人工智能应用程序和数据。
- 设备证明的可验证性：确保只有授权的最终用户才能在 H200 或 H100 的 TEE 中放置要执行的数据和代码。此外，设备认证可验证用户是否正在与真实的 NVIDIA GPU 进行通信、固件是否未被篡改以及 GPU 固件是否已按预期更新。
- 无需更改应用程序代码：在大多数情况下， GPU 加速工作负载无需更改代码，即可充分利用机密计算的所有优势，同时保持安全性、隐私性。


## 安全可信的LLM

在 TEE 中保护用户隐私的同时，需要平衡性能需求，TEE 和异构硬件的结合将成为未来发展趋势。 随着 CPU 性能的提升以及内置 AI 加速技术的升级和更新，在方便部署的场景下，CPU 会是大模型推理和 TEE 结合的首选；在训练的场景下，基于 CPU 的 TEE 结合异构硬件的加密支持，则会是大模型训练甚至大模型联邦训练的技术方向。

- 中国信通院《大模型可信赖研究报告》提出了大模型可信赖六大目标，包括可靠性、健壮性、安全性、公平性、可问责和可解释，同时提出大模型可信风险来自于框架、数据、模型和生成内容等四大方面。
- AI数据标注服务是大模型生命周期中的重要一环。Appen是一家全球领先的AI数据标注服务商，提供AI数据采集和数据标注服务。Appen在全球拥有百万数据标注众包资源，该公司在语音数据标、图像数据标注、自动驾驶数据标注等领域具有独特优势。进入大模型时代，Appen也推出了基于人类反馈的强化学习等数据和AI服务，用于解决大语言模型中存在偏见和幻觉等风险，还推出了自动化NLP数据标注，利用生成式AI功能和零样本/少样本学习技术加快数据标注
- 大模型AI安全可信，是一组相关数据、技术、产品、方案和服务等组合起来的能力集合，其中包括：数据采集、标注和清洗等数据服务；模型设计、评估与微调等模型服务；基于人类反馈的增强学习服务；数据保护与隐私安全；咨询服务等等诸多领域，涉及到了大模型全生命周期的方方面面。
- 在大模型公有云服务方面，以百度、阿里等为代表的互联网与云服务公司，从大模型全生命周期视角出发，涵盖大模型训练、精调、推理、大模型部署、大模型运营等关键阶段面临的安全风险与业务挑战，在自有技术体系内进行深入布局，探索打造安全产品与服务。
- 360等第三方独立的人工智能与安全科技公司，探索“以模型管理模型”方式，打造以大模型为核心的AI Agent（AI智能体）,带入企业真实安全运营场景中，以“虚拟安全专家”的形象，满足企业对安全业务的需求。
- IBM等公司推出了独立的AI安全可信工具或组件，专门用于检查数据集、机器学习模型、业界最新算法等是否具有偏见；还在一些组件中专门集成了AI模型风险管理以及数据治理和数据质量管理等功能。
- 开源社区也积极推出大模型安全可信开源工具。[LLM Guard](https://github.com/protectai/llm-guard)是一个保护和强化大型语言模型安全性的开源工具包，LLM Guard的目的是通过提供开箱即用的所有必要工具来简化公司安全采用大模型的过程。

## 大模型风险分析

### 大模型风险视图

大模型快速部署和广泛应用的同时，也诱发了更多的风险隐患：

- 框架风险，深度学习框架面临物理、网络层面的恶意攻击，导致大模型所依赖的基础设施稳定性和安全性难以保障；
- 数据风险，采集及处理海量、多模态的训练数据可能会引入更多的有害数据，容易引发个人隐私泄露、知识产权侵权、数据偏见等问题；
- 模型风险，现阶段，大模型抗干扰能力相对较弱，存在遭受恶意攻击、决策偏见以及模型运营风险等问题；
- 生成内容风险，大模型存在“幻觉”现象，答非所问、违规不良信息生成等问题成为大模型最受关注的风险。大模型高效、便捷的内容生成能力大幅降低了诈骗、钓鱼邮件等恶意行为的门槛，而针对生成内容的追溯保障机制目前尚未完善，使得恶意内容生成的监管更加困难。

**大模型可信赖目标：可靠性，健壮性，安全性，公平性，可问责，可解释**

**大模型全生命周期治理理念：数据采集，模型预训练，模型微调，部署运行，优化更新**

![](/img/post_pics/cc/LLM-security-1.png)

### 框架软件漏洞是现有深度学习框架短板

大模型领域的基础设施风险主要包括深度学习框架和开发套件等软件层面的漏洞，以及运行环境的不稳定性。可能的风险涵盖物理攻击、网络攻击、运行环境篡改、运维故障等多个方面。

在大模型训练阶段，深度学习框架、开发组件以及第三方依赖库存在潜在漏洞，增加了受到外部恶意攻击的风险。在这个阶段，攻击者有可能通过恶意程序入侵等手段，窃取模型、训练数据以及训练脚本等核心资产，从而导致大模型的训练数据和模型参数文件的泄露。早在2020年9月，TensorFlow就被曝出多项安全漏洞，其中危险等级严重的漏洞2个，高危漏洞8个，中危漏洞12个，低危漏洞2个。这些漏洞可能导致任意代码执行、信息泄露以及拒绝服务等。

**深度学习框架的运行环境容错性低，核心资产保护面临挑战。**大模型的运行环境不稳定性风险主要来自大模型服务的运维以及模型迭代更新时稳健性较差所导致的服务等级协议（SLA）服务水平不足，从而可能影响大模型服务可用性。在训练和推理过程中，由于设备、网络或通信故障，可能导致模型训练或推理任务中断。此外，大模型的运行环境同样面临安全性风险。一方面，缺乏基础设施与其他系统的严格网络隔离可能导致来自内部其他系统的横向渗透风险。如果攻击者成功侵入基础设施系统并注入后门、木马等恶意程序，整个系统将面临严重的安全风险。另一方面，大模型的运行环境缺乏面向训练数据、模型和网络通信的安全防护措施，使得训练数据、模型参数文件等核心资产容易受到泄露、篡改和窃取等威胁。

针对深度学习框架面临的软件漏洞风险与运行环境不可靠问题，一方面通过采用漏洞管理、恶意程序检测以及访问控制等技术措施，降低深度学习框架受恶意访问和攻击的可能性，另一方面通过构建AI核心资产保护机制，保障深度学习框架运行环境的安全可信。

#### 可信赖框架降低恶意访问与攻击风险

可信赖框架的实现需要从框架自身管理层面、框架外的平台层面以及用户管理层面进行安全保障。

- **安全漏洞管理机制**通过对AI框架进行定期的漏洞扫描，识别并记录框架漏洞信息，定时更新安全补丁修复漏洞，提升框架安全能力。
- **恶意程序检测机制**通过将检测模块直接集成在深度学习框架或者基础设施中，实现检测在训练或者推理任务执行的容器或虚拟机是否存在恶意攻击宿主机、宿主机上其他容器或者执行越权访问等容器逃逸行为。判别是否存在勒索病毒以及恶意程序，并产生告警信息。
- **访问控制和身份鉴别机制**有效管理并核验登录用户的真实身份，对于多次登录失败的用户，应启用结束会话、限制非法登录次数等措施，以降低未授权操作所引发的风险。

#### 核心资产保护机制保障运行环境安全可信

为保障深度学习框架的运行环境安全可信，通过构建加解密机制、完整性校验机制、训练任务中断恢复机制以及运行环境隔离机制等方式保障运行过程中AI核心资产的安全。

- **加解密机制**通过在深度学习框架和人工智能基础设施中添加加解密模块，实现对训练和推理过程中的数据和模型参数文件等AI核心资产进行保护，防止未授权人员进行非法访问、篡改数据。
- **完整性校验机制**通过对数据和模型相关文件进行完整性校验，提升大模型在预训练、微调以及后续部署运行阶段的可靠性，通过密码算法或者完整性校验机制对数据和模型参数文件进行加解密处理，核验各阶段的文件完整性。
- **训练任务中断恢复机制**可以在故障发生后及时保存训练任务上下文及模型参数等信息，并且可支持在新的训练节点加载训练任务上下文及模型参数等信息，正常恢复原始训练任务，大幅提升大模型在训练阶段的可靠性。
- **运行环境隔离机制**通过设置独立的安全区域保障AI资产在训练和推理过程中的安全性。以可信执行环境技术（TEE）为例，TEE是处理器中一个独立的安全区域，用于保护程序与数据的机密性和完整性不被外部窃取和破坏。与存储加密和网络通信加密一起，TEE可以保护落盘和通信过程中的数据隐私和安全。随着TEE技术的发展，在计算核心与内存之间增加安全处理器，以保护被计算核心使用的数据安全和隐私的机密计算技术出现。

### 数据层面，隐私风险与有害数据导致模型不可靠

大模型的训练依赖于大规模、多样化且高质量的数据集。这些训练数据通常涵盖各类网页、公共语料库、社交媒体、书籍、期刊等公开数据来源，其中未经筛选和审核的数据成为大模型不可忽视的潜在风险。因此，在大模型的全新范式下，数据来源不可信、数据违规处理、投毒攻击、数据内容有害、数据偏见、数据样本不足正逐步成为大模型在数据方面的主要风险。

大模型训练数据的采集、预处理等数据处理活动可能涉及数据来源管理困难、隐私泄露等相关风险。在数据来源管理方面，主要问题集中在数据来源的不可靠性和不可追溯性。大模型训练数据通常涵盖图像、视频、文本、音频等多种数据类型，涉及自采集、商业采购、公开数据集等多种渠道。然而，部分公开数据集的来源缺乏充分的验证和审核，导致预训练数据集中存在来源不清、被恶意投毒的数据。大量训练数据采集的同时难以避免带毒数据的引入，增加了数据来源管理的难度。

在隐私泄露方面，数据采集阶段可能会由于采集方式、采集工具的不合规，导致未获取个人信息授权，使得预训练数据集含有未授权个人信息。在数据预处理阶段，由于数据脱敏机制的不完善，个人信息未完全去标识化，致使预训练模型学习、理解到含有个人信息的知识，其生成内容可能会含有个人信息或关联个人信息，存在个人信息泄露的风险。

**有害内容、低质量数据导致模型生成违规内容。**大模型通过学习海量数据中的知识、理解常识并生成内容，数据中存在有害内容和数据偏见等质量问题可能导致模型生成内容存在违规信息或决策偏见等问题。

在数据内容有害性风险方面，模型预训练阶段使用大量无监督学习预训练数据集，如果其中存在一定量的有害内容，将影响预训练模型的理解和生成能力。同时，在模型微调阶段，微调数据若包含不准确、虚假信息等内容，可能导致模型无法正确对下游任务模型进行价值对齐。数据偏见风险主要源自大模型的预训练和微调阶段。一方面，模型预训练所使用的数据集样本分布可能缺乏均衡性，包括性别、民族、宗教、教育等相关样本比例关系不当。另一方面，模型微调阶段可能由于人工标注员的主观意识形态偏差，引入对微调数据的构建和价值排序的偏见，从而导致微调数据存在价值观上的偏见歧视问题。

数据的使用贯穿大模型全生命周期，安全保障与有效处理是保障大模型可靠的关键举措。在数据层面，可信赖实践主要涉及数据全流程的安全合规处理、数据安全沙箱技术、投毒检测以及数据分析等措施。

#### 安全合规的数据处理机制降低数据处理风险

大模型的数据处理活动主要包含数据采集、数据预处理及模型训练等环节。

- 在数据采集环节，通常会建立数据采集来源管理、数据采集业务评估、数据采集审批流程、采集合规审批等管理机制，确保数据采集的合规性、正当性和执行上的一致性。针对数据来源问题，知识产权部门和信息安全部门协助业务部门对数据来源信息的合理性、正当性进行审查，去除含有大量不良违法信息的有害数据来源，并对数据来源信息进行备案管理。
- 在数据预处理环节，数据处理人员会将收集到的原始数据进行清洗、去重、格式化等多步骤的预处理以确保数据质量。在该过程中，数据处理人员会严格筛查，去除那些不完整、错误、带毒或含有敏感信息的数据。随后数据处理人员通过自动化工具和人工相结合的方式，对预处理后的数据进行标注和筛选，以识别训练数据中是否包含敏感信息。此外，业务部门通过构建敏感内容反馈机制，利用生成内容自身特性，将敏感内容作为负面样本训练敏感信息鉴别模型，持续提升模型性能。
- 在大模型训练阶段，通常会首先进行个人信息安全影响评估，确保大模型的研发和运营过程满足现有个人信息保护的合规要求。通过核对个人信息保护评估清单，推动面向个人信息保护的产品功能设计，确保人工智能产品设计流程合规，保障数据收集和处理（包括使用、披露、保留、传输和处置）限于所确定的必须的目的。

#### 数据安全沙箱技术实现数据可用不可见

数据安全沙箱是一项通过构建可隔离、可调试、运行环境安全等功能来分离数据、模型使用权和所有权的技术。在大模型微调场景中，数据拥有方可通过沙箱客户端将数据通过加密信道上传到沙箱中，随后通过数据安全沙箱对加密数据进行预处理和模型微调，并通过安全信道反馈微调后的模型，保证了模型拥有方的预训练模型不出私有域的前提下，数据拥有方可以安全的完成模型微调任务。

![](/img/post_pics/cc/LLM-security-2.png)

#### 投毒检测与数据分析识别有害内容

- 在数据投毒检测方面，通过数据去毒工具在数据预处理环节检测训练数据是否存在异常。数据投毒检测可采用多种不同的检测手段。基于规则、关键词进行检测是一种常见但有效的方式，可在丰富完善检测规则的基础上，以较高的效率将被投毒的、危害安全的训练数据进行截获去除。也可采用传统语言模型或大语言模型的手段，针对数据投毒问题进行相应的设计和优化，通过语义相似度等指标进行检测，从而判定出更隐蔽、更难以察觉的数据安全问题。
- 在数据分析工具方面，可采用分类统计、向量聚类、大模型识别等方法，对数据内容门类、语料形式、语料来源、作者等数据分布进行统计和分析，使参与到模型预训练中的训练数据配比均匀、优质来源和优质形式的数据占比较高，修正性别、民族、宗教、教育等统计偏见，使模型在运营阶段避免可能存在的安全性、公平性等问题。

### 模型层面，提示词攻击诱发模型脆弱性风险

大模型在模型开发和运营阶段都会面临多种模型内外部的风险，主要包括提示注入攻击等安全性问题、健壮性不足、偏见歧视以及模型运营风险等问题。提示注入攻击成为大模型安全性首要风险。

**提示注入攻击是一类以输入提示词作为攻击手段的恶意攻击。**攻击者精心构造和设计特定的提示词，达到绕过大模型过滤策略的目的。根据窃取目标和攻击手段不同，可将提示注入攻击细分为以下三类。

- 目标劫持，攻击者通过输入恶意示例的方式劫持模型的输出结果，并要求模型输出与其原输出内容不同的特定结果，从而恶意篡改生成内容。
- 提示泄露，攻击者通过一些诱导性的上下文提示，窃取大模型预制的初始化提示内容，包括模型应该遵循的规则和特定敏感话题。攻击者可以通过该类攻击手段了解大模型的行为模式或者过滤策略。
- 越狱攻击，攻击者通过模拟对话、角色扮演等虚构场景和行为方式，设定一系列特定的问答规则，尝试分散大模型的注意力，规避过滤策略，生成带有恶意目的的特定输出结果。
 
除直接对大模型的输入内容进行提示注入攻击，攻击者也可以通过文件中内嵌恶意代码等形式间接进行提示注入攻击。以微软NewBingChat为代表的大模型，其结合检索和API调用功能的新组件引入了间接提示注入的风险。攻击者有可能通过在提示词中嵌入含有恶意代码或有害内容的网页链接或文件等手段，试图规避输入和输出端的过滤机制，以生成特定的恶意内容。

**大模型在健壮性和泛化性方面仍然面临挑战。**与传统的小参数量机器学习模型相比，虽然大模型通过使用亿级参数的训练数据进行无监督学习表现出对抗样本攻击和外部干扰的相对强健性，但仍存在健壮性和泛化性不足的潜在风险。例如，在大模型的输入提示词中引入一定程度的错别字符或文字、逻辑错误的词句以及段落等内容，会导致大模型理解偏差以及生成内容错误。

**大模型的决策偏见歧视问题愈发突出。**大模型的算法决策公平性是可信赖能力的重要指标，尤其在金融、医疗、教育等特殊行业中，这一指标对于处理关键问题的理解和生成任务至关重要。首先，预训练数据自带的偏见歧视会导致预训练模型进一步放大偏见问题，长尾问题仍然是潜在偏见之一。其次，大模型本身可能根据数据样本的分布和属性，进一步提升对某类样本的敏感度，从而间接放大对这些偏见性知识的感知，进而导致更为严重的歧视性内容生成。

**大模型运营面临多方面挑战，API安全问题至关重要。**当前，模型即服务（MaaS）等高效而敏捷的部署方式正逐步成为现有大模型系统与服务的主流形式。一方面，在大模型服务实际运营环节，存在诸多服务运营相关的风险，包括但不限于批量注册、盗号、撞库等账号安全性问题，以及恶意使用、机器作弊、审核资源浪费等运营安全性问题。以ChatGPT为例，该服务推出仅两个月，注册用户已超过1亿。随着用户规模不断增长，各类违规账号也在不断活跃。于是自2023年4月起，OpenAI大规模封禁各类违规注册账号。另一方面，大模型主要通过API提供对外服务。在服务运营阶段，攻击者可能通过注入漏洞利用攻击、未授权漏洞利用攻击、越权访问漏洞利用攻击、代码设计漏洞攻击以及第三方组件漏洞利用攻击等方法，引发API崩溃、数据泄露以及拒绝服务等严重问题。例如，研究人员发现通过提示词混合Python代码的模板函数可以利用大模型应用框架LangChain的接口远程执行任意Python代码。

在模型层面，可信赖实践可从设计开发、模型训练和部署运行三个阶段展开。设计开发阶段主要涉及大模型研发前期的安全和伦理设计评估；在模型训练阶段，主要涉及大模型预训练、微调过程的可信赖能力检测、加固措施；在部署运行阶段，主要涉及大模型在运营过程中的运维能力，以增强用户对于模型运营的信任度。

#### 安全和伦理设计评估为大模型研发提供全方位保障

大模型的安全性设计评估是面向大模型设计初期的一项安全性评审工作，主要涉及安全审核和安全功能设计两方面。
- 在安全审核方面，通常会根据大模型设计需求构建威胁模型，并生成安全设计核查表对大模型安全性设计进行评审，保障大模型的设计需求满足安全合规要求。
- 在安全功能设计方面，大模型研发人员会根据安全审核结果，对大模型进行安全功能设计，包括但不限于生成内容过滤机制、生成内容标识、投诉反馈功能等。
- 大模型的伦理设计评估主要依据人工智能伦理治理相关法律法规和标准文件，面向数据、算法以及应用管理风险三方面，围绕产品设计、开发、部署、运营的全生命周期，分阶段、分目标的对大模型伦理风险进行分类分级管理，并根据风险的等级进行内部自评估以及外部专家评审，以确保大模型的训练数据、决策机制以及生成内容符合伦理道德。目前，针对大模型伦理评估工作，商汤建立了覆盖产品全生命周期的风险控制机制，初步形成了大模型的伦理治理闭环。通过建立数据风险、算法风险以及应用风险三方面的伦理评估机制，对产品设计、开发、部署、运营的全生命周期实施分阶段、分目标的伦理风险分类分级管理，并建立了配套的风险自查、评估、审查和跟踪审查流程。

#### 评测与对齐是模型训练可信赖的关键技术措施

大模型的模型评测和对齐技术是目前解决模型安全性、健壮性、公平性不足的主流方法，通过将评测结果作为奖励模型的反馈优化数据，对模型进行针对性的微调与对齐，大模型能够在模型层面更可靠、可信。

**大模型可信赖评测是提升模型抵抗外部恶意攻击、干扰信息以及决策偏见的重要手段。**大模型可信赖的重点评测对象是安全性、健壮性以及公平性。

- 在安全性测试方面，评测人员通常采用对抗性提示的方式对大模型进行目标劫持、提示泄露以及越狱等安全性评测。
- 在健壮性测试方面，评测人员通常会采用错别字、同义替换、无关提示、修改语义等方式，对生成内容的一致性、稳定性进行评测。
- 在公平性测试方面，评测人员会根据模型业务特性，针对年龄、国家、性别、种族等敏感属性进行公平性评测，通过比对输入内容中是否含有敏感属性的输出结果差异，统计模型的公平性表现。在评测完成后，评测人员会协同研发人员共同构建面向安全性、健壮性和公平性的模型加固方案，包括但不限于增量学习、设计针对性的微调提示问答对、增强奖励模型的针对性训练等。

**思维链技术有效提升模型逻辑表达能力。**为保障大模型的生成内容具备更加合理的推理性逻辑表达，微调阶段的标注人员可通过思维链技术，在同一提示词中引入多项解释性示例，引导模型生成具备一定推理逻辑的回答。比如，在数理逻辑任务中，可在示例部分编写步骤分解形式的解释说明内容，指导模型更容易生成推理步骤清晰，准确性高的回答内容。

![](/img/post_pics/cc/LLM-security-3.png)

**人类反馈强化学习（RLHF）是现阶段大模型对齐研究的主要方法。**RLHF是一项通过人工反馈回答内容的好坏顺序指引大模型的价值观与人类对齐的技术。目前，包括OpenAI、谷歌、百度、商汤科技等主流大模型均采用了RLHF技术对大模型进行价值对齐调优。比如，商汤科技已经将模型评估测试与RLHF技术结合，将相关测试结果反馈于模型强化学习的过程之中，帮助进一步提升大模型风险防御能力。

#### 投诉反馈、风险监控以及应急处置构建模型运营能力

投诉反馈机制是针对大模型生成内容优化更新的重要手段。目前投诉反馈机制主要是通过成立投诉反馈监管治理机构，对所有的不良违法生成内容进行处理。为了更好的推动模型的持续优化，模型更新的研发人员会定期对生成内容的投诉和举报进行分析和总结，以便发现问题的根源，并采取措施防止类似问题再次发生。

风险监控有效助力大模型良性运营。在模型运营能力建设方面，运营人员会持续对大模型的运营情况进行风险监控并对有害内容进行溯源，通过对大模型记录的用户上传内容、用户上传时间、IP地址、设备信息等信息进行核查，可实现对该内容的制作者和使用者进行追溯。

应急处置用户恶意行为抑制有害内容生成与传播。大模型运营期间运营人员会对用户异常行为、违规用户帐号进行监控处置。针对用户异常行为，运营人员通过对用户行为进行分析，根据异常活跃度、登录情况以及输入内容进行判断处置。针对违规用户帐号，运营人员通过帐号管理功能实现对恶意用户的限期改正、暂停使用、终止帐号等措施，防止有害内容的进一步生成和二次传播。

### 生成内容层面，安全风险和不可追溯是重点难题

当前，大模型的生成内容中仍然存在一定程度的内容安全和不可追溯风险，主要包括虚假有害内容、上下文逻辑性错误、问答与提问的相关性较差、与社会主流价值观冲突等风险，进一步降低了以大模型为生产工具的恶意行为的门槛，对个人、组织以及社会的稳定发展造成严重影响。其主要风险包括以下几方面：

**生成内容“幻觉”现象频发。**大模型对输入的问题生成不真实、与现实世界常识相违背的虚假有害信息的现象，被称为“幻觉”问题。大模型常见的幻觉主要有三类：第一是和用户输入冲突的幻觉，大模型的理解能力极大依赖于训练数据集的规模、种类、样本的丰富度，理解能力的不足将会导致大模型无法准确生成用户输入的问题答案，影响大模型的生成内容可信度。第二是和已生成的上下文冲突的幻觉，尽管目前大模型具备广泛的世界知识，但其仍是一个黑盒、逻辑推理不够精确的系统。大模型通过理解输入内容的token，预测并逐字逐句生成输出结果，其生成的内容虽符合训练数据中语句的表达连贯性，却可能缺乏合理、清晰的逻辑性，与上下文内容冲突或生成重复性内容。第三是和事实知识冲突的幻觉，这一类幻觉的研究难度更大，对用户实际使用体验的干扰也最大。例如，大模型在生成医疗建议时可能会捏造错误的药品剂量，误导缺少专业医学知识的用户，直接危及用户健康。

**生成内容与社会主流价值观冲突。**大模型的生成内容的安全性问题至关重要，如果大模型生成民族仇视、偏见和歧视、政治和军事敏感、淫秽色情以及恐怖暴力等恶意内容，会对传统道德和社会核心价值观造成冲击，对个人、组织和社会都具有极其严重的负面影响。

**生成内容欠缺合理、科学的推理过程。**目前大模型的可解释性问题仍然研究学者重点关注的方向，针对大模型的可解释性研究主要分为事前解释和事后解释，其中事前解释是通过研究不同特征对预测结果的影响程度进行解释说明，事后解释更加侧重利用规则以及可解释性强的算法评估原有大模型的可解释性。然而，大模型所使用的训练数据和算法结构仍然是黑盒，难以完全解释目前大模型的内在机理和决策依据。

**生成内容不易追溯和保护。**大模型由于具备通过学习海量的世界知识生成内容的能力，因此在训练数据和生成内容方面会产生一系列的版权归属和保护难题。目前大模型服务通常会采用数字水印技术在生成内容中嵌入不可见、具备可追溯能力的标识，该类标识一般内含用户ID信息、大模型服务信息以及时间戳等信息，用于追溯不良违规生成内容，但目前仍然面临生成内容被二次创作、剪辑和裁切之后，标识内容可能会无法读取等问题，导致无法正确追溯到原始的大模型服务，难以明确界定责任归属。在知识产权的溯源方面，由于现有大模型的学习机制，其生成的内容有可能与原始的训练数据具有一定相似度，难以界定生成的内容是否对原始作品产生侵权行为。

**生成内容误用滥用现象对个人、团体以及社会造成不良影响。**由于目前仍然缺乏对于使用大模型生成能力的有效监督手段，部分用户在未充分进行培训和教育的前提下，可能将隐私信息误输入到大模型中，导致个人信息泄露。例如，2023年3月，三星半导体部门员工因三起利用ChatGPT处理办公文件和修复程序源代码等事件，导致公司机密泄露。部分恶意使用者利用FraudGPT等恶意大模型作为违法活动的工具生成诈骗短信和钓鱼邮件，通过代码生成工具开发恶意程序、脚本等，窃取他人敏感个人信息。

在生成内容方面，可信赖实践主要涉及生成内容评测、内容审核机制以及内容可追溯能力的建设，实现内容安全可控并具备一定程度的可追溯能力。为缓解大模型“幻觉”现象，生成内容评测主要聚焦真实性、准确性以及安全性。为降低生成内容的安全性风险，内容审核机制通常会采取机器审核和人工复审结合的形式。为进一步提升二次编辑导致生成内容难以追溯的问题，数字水印技术正在逐渐提升健壮性能力。

#### 生成内容评测为模型优化更新提供反馈样本

生成内容真实性测试抑制深度合成图像等恶意攻击。评测人员可通过内容真实性测试检测图像中面部表情一致性与动作序列连贯性，并结合频谱、声音和文字等多模态信息，准确鉴别包括图像编辑、换脸、活化以及各种先进扩散模型合成的人像图像。

生成内容准确性测试客观反馈大模型“幻觉”水平。在生成内容准确性测试方面，评测人员可采用人工打分或自动化评估等形式，对生成内容的质量进行评估，目前商汤科技主要采用整体评价、相关性、可读性、拟人性、专业性等五个指标对文本生成质量进行评价，并从生成内容事实性错误，生成内容逻辑性错误，生成内容和问题相关性错误等三个方面对文本生成准确性进行评价。

生成内容安全性评测守卫大模型生成内容红线。在生成内容安全性测试方面，评测人员可采用“红队测试”的方法，通过构建恶意问题数据集对生成内容安全性进行评测，其评测的维度包括但不限于身心健康、隐私财产、伦理道德、偏见歧视、违法犯罪、政治敏感等话题。

#### 内容审核机制有效过滤有害输入及输出内容

大模型的生成内容审核机制主要由机器审核和人工复审构成。机器审核是一种对大模型有害输入、输出内容进行检测、识别的机制，可以有效识别并过滤有害、不准确、不恰当的内容，通常采用关键词和语义分析等技术。人工复审机制是目前实现大模型生成内容安全的重要保障。通过人工复审的方式，对大模型输入、输出的内容进行再次核验。人工复审需记录审核时间、审核覆盖度、抽检方式、审核处置结论等信息。除人工复审机制外，还可以采用巡查审查等方式，定期对经过了机器审核、人工复审的内容进行整体巡查，并及时根据巡查结果优化调整审核规则及策略。巡查审核需记录审核时间、审核覆盖度、抽检方式、审核处置结论等信息。

![](/img/post_pics/cc/LLM-security-4.png)

#### 健壮性数字水印助力实现内容可追溯可问责

数字水印技术是一种将信息嵌入到数字媒体（如图像、音频和视频）中的技术，以便在不改变原始媒体质量的前提下，对其进行标识或保护。这种技术目前被广泛应用于版权保护、内容认证和数据管理等领域。数字水印的健壮性是指其在面对压缩、滤波、剪切、旋转、缩放等攻击时仍能被正确检测的能力。为保障生成内容的可追溯性，通常会采用纠错编码、多重水印、深度学习等水印嵌入方案进一步提升数字水印的健壮性。

![](/img/post_pics/cc/LLM-security-5.png)

## 附录

[2023 大模型可信赖研究报告](/pdf/2023大模型可信赖研究报告.pdf)
[可信执行环境保障大模型安全](https://blog.csdn.net/qq_43543209/article/details/135683986)
[如何构建安全可信的AI大模型](https://mp.weixin.qq.com/s/6hdBb0fDQUZIQiHjEnpG5g)
[用基于英特尔SGX 的可信执行环境有效应对大语言模型隐私和安全挑战](https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/privacy-security-challenge-large-language-model.html)
[维基百科 Confidential computing 机密计算](https://en.wikipedia.org/wiki/Confidential_computing)
[维基百科 安全多方计算](https://en.wikipedia.org/wiki/Secure_multi-party_computation)
[机密计算联盟](https://confidentialcomputing.io/resources/white-papers-reports/)
[7 个服务框架 LLMs](https://betterprogramming.pub/frameworks-for-serving-llms-60b7f7b23407)
[保护 Kubernetes 工作负载的安全：签名和加密容器映像的实用方法](https://itnext.io/securing-kubernetes-workloads-a-practical-approach-to-signed-and-encrypted-container-images-ff6e98b65bcd)
[大型语言模型的机密容器](https://pradiptabanerjee.medium.com/confidential-containers-for-large-language-models-42477436345a)
[LLM Guard - 大规模语言模型交互的安全卫士](https://blog.csdn.net/gitblog_00056/article/details/139085060)
